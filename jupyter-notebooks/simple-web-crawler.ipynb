{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple web crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dependency packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.5.1-py2.py3-none-any.whl (254 kB)\n",
      "Collecting h2<4.0,>=3.0\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.6.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\sajjad\\anaconda3\\lib\\site-packages (from scrapy) (2.2.2)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\sajjad\\anaconda3\\lib\\site-packages (from scrapy) (18.0.0)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting Twisted[http2]>=17.9.0\n",
      "  Downloading Twisted-21.2.0-py3-none-any.whl (3.1 MB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading zope.interface-5.4.0-cp36-cp36m-win_amd64.whl (210 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.5.zip (47 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.1.16.tar.gz (3.2 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.4.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\sajjad\\anaconda3\\lib\\site-packages (from scrapy) (4.2.1)\n",
      "Requirement already satisfied: idna>=2.1 in c:\\users\\sajjad\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (2.6)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\users\\sajjad\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (0.24.0)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\sajjad\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.11.0)\n",
      "Requirement already satisfied: cffi>=1.7 in c:\\users\\sajjad\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.11.5)\n",
      "Collecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting pyasn1-modules\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting pyasn1\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting attrs>=19.1.0\n",
      "  Downloading attrs-21.4.0-py2.py3-none-any.whl (60 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading incremental-21.3.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-20.2.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting twisted-iocpsupport~=1.0.0\n",
      "  Downloading twisted_iocpsupport-1.0.2-cp36-cp36m-win_amd64.whl (44 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Collecting priority<2.0,>=1.1.0\n",
      "  Downloading priority-1.3.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sajjad\\anaconda3\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (39.1.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sajjad\\anaconda3\\lib\\site-packages (from cffi>=1.7->cryptography>=2.0->scrapy) (2.18)\n",
      "Building wheels for collected packages: protego, PyDispatcher\n",
      "  Building wheel for protego (setup.py): started\n",
      "  Building wheel for protego (setup.py): finished with status 'done'\n",
      "  Created wheel for protego: filename=Protego-0.1.16-py3-none-any.whl size=6802 sha256=afb2afd7d1b06481187fb4af13d1781e8f78de722782a58e5acf71bf952d9e2a\n",
      "  Stored in directory: c:\\users\\sajjad\\appdata\\local\\pip\\cache\\wheels\\b2\\74\\25\\517a0ec6186297704db56664268e72686f5cfa8ab398582f33\n",
      "  Building wheel for PyDispatcher (setup.py): started\n",
      "  Building wheel for PyDispatcher (setup.py): finished with status 'done'\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-py3-none-any.whl size=11527 sha256=b3dc2547ac446bb0447eb3d5da2c27a795232cc1b4425ac885e57f0a2bd9e7d4\n",
      "  Stored in directory: c:\\users\\sajjad\\appdata\\local\\pip\\cache\\wheels\\69\\20\\28\\fbdcea83fadaf56e6e3ed24df2a1b409a8d950d3cce69bbfce\n",
      "Successfully built protego PyDispatcher\n",
      "Installing collected packages: attrs, zope.interface, w3lib, twisted-iocpsupport, pyasn1, incremental, hyperlink, hyperframe, hpack, cssselect, constantly, Automat, Twisted, pyasn1-modules, priority, parsel, jmespath, itemadapter, h2, service-identity, queuelib, PyDispatcher, protego, itemloaders, scrapy\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 18.1.0\n",
      "    Uninstalling attrs-18.1.0:\n",
      "      Successfully uninstalled attrs-18.1.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 0.9.3\n",
      "    Uninstalling jmespath-0.9.3:\n",
      "      Successfully uninstalled jmespath-0.9.3\n",
      "Successfully installed Automat-20.2.0 PyDispatcher-2.0.5 Twisted-21.2.0 attrs-21.4.0 constantly-15.1.0 cssselect-1.1.0 h2-3.2.0 hpack-3.0.0 hyperframe-5.2.0 hyperlink-21.0.0 incremental-21.3.0 itemadapter-0.4.0 itemloaders-1.0.4 jmespath-0.10.0 parsel-1.6.0 priority-1.3.0 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.8 queuelib-1.6.2 scrapy-2.5.1 service-identity-21.1.0 twisted-iocpsupport-1.0.2 w3lib-1.22.0 zope.interface-5.4.0\n"
     ]
    }
   ],
   "source": [
    "## scraper packages\n",
    "!pip install scrapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "\n",
    "We use the package `scrapy` for scraping data from web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class BrickSetSpider(scrapy.Spider):\n",
    "    name = 'brick_spider'\n",
    "    start_urls = ['http://brickset.com/sets/year-2016']\n",
    "\n",
    "    def parse(self, response):\n",
    "        SET_SELECTOR = '.set'\n",
    "        for brickset in response.css(SET_SELECTOR):\n",
    "\n",
    "            NAME_SELECTOR = 'h1 ::text'\n",
    "            PIECES_SELECTOR = './/dl[dt/text() = \"Pieces\"]/dd/a/text()'\n",
    "            MINIFIGS_SELECTOR = './/dl[dt/text() = \"Minifigs\"]/dd[2]/a/text()'\n",
    "            IMAGE_SELECTOR = 'img ::attr(src)'\n",
    "            yield {\n",
    "                'name': brickset.css(NAME_SELECTOR).extract_first(),\n",
    "                'pieces': brickset.xpath(PIECES_SELECTOR).extract_first(),\n",
    "                'minifigs': brickset.xpath(MINIFIGS_SELECTOR).extract_first(),\n",
    "                'image': brickset.css(IMAGE_SELECTOR).extract_first(),\n",
    "            }\n",
    "\n",
    "        NEXT_PAGE_SELECTOR = '.next a ::attr(href)'\n",
    "        next_page = response.css(NEXT_PAGE_SELECTOR).extract_first()\n",
    "        if next_page:\n",
    "            yield scrapy.Request(\n",
    "                response.urljoin(next_page),\n",
    "                callback=self.parse\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-06 23:02:58 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2022-02-06 23:02:58 [scrapy.utils.log] INFO: Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Windows-10-10.0.19041-SP0\n",
      "2022-02-06 23:02:58 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2022-02-06 23:02:58 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2022-02-06 23:02:59 [scrapy.extensions.telnet] INFO: Telnet Password: 5e18c325b43c54a3\n",
      "2022-02-06 23:02:59 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2022-02-06 23:02:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-02-06 23:02:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-02-06 23:02:59 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-02-06 23:02:59 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-02-06 23:03:00 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-02-06 23:03:00 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-02-06 23:03:00 [scrapy.core.engine] DEBUG: Crawled (403) <GET http://brickset.com/sets/year-2016> (referer: None)\n",
      "2022-02-06 23:03:00 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <403 http://brickset.com/sets/year-2016>: HTTP status code is not handled or not allowed\n",
      "2022-02-06 23:03:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2022-02-06 23:03:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 226,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 2091,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/403': 1,\n",
      " 'elapsed_time_seconds': 0.339443,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 2, 7, 3, 3, 0, 346409),\n",
      " 'httpcompression/response_bytes': 3137,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'httperror/response_ignored_count': 1,\n",
      " 'httperror/response_ignored_status_count/403': 1,\n",
      " 'log_count/DEBUG': 1,\n",
      " 'log_count/INFO': 11,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2022, 2, 7, 3, 3, 0, 6966)}\n",
      "2022-02-06 23:03:00 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    \"FEEDS\": {\n",
    "        \"items.json\": {\"format\": \"json\"},\n",
    "    },\n",
    "})\n",
    "\n",
    "process.crawl(BrickSetSpider)\n",
    "process.start() # the script will block here until the crawling is finished"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0c48e2bddb07972c5947abb8e7c3bc6b78fe2c14f866bbaa314f904d72cdae0"
  },
  "kernelspec": {
   "display_name": "Python 3.6.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
